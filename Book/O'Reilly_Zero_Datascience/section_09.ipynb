{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#9章　データの取得\n",
    "\n",
    "#9.1\n",
    "\n",
    "import sys, re\n",
    "\n",
    "regex = sys.argv[1]\n",
    "\n",
    "for line in sys.stdin:\n",
    "    if re.search(regex, line):\n",
    "        sys.stdout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.2\n",
    "\n",
    "#重要：csvファイルを扱う際には、rかwの後ろにaを置いて、常にバイナリモードでopenするべき。\n",
    "\n",
    "import csv\n",
    "\n",
    "def process(d, s, c):\n",
    "    print(d, s, c)\n",
    "\n",
    "with open('tab_delimited_stock_prices.txt', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        date = row[0]\n",
    "        symbol = row[1]\n",
    "        closing_price = float(row[2])\n",
    "        process(date, symbol, closing_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('colon_delimited_stock_prices.txt','r') as f:\n",
    "    reader = csv.DictReader(f, delimiter=':')\n",
    "    for row in reader:\n",
    "        date = row['date']\n",
    "        symbol = row['symbol']\n",
    "        closing_price = float(row['closing_price'])\n",
    "        process(date, symbol, closing_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "today_prices = { 'AAPL':90.91, 'MSFT':41.68, 'FB':64.5 }\n",
    "\n",
    "with open('comma_delimited_stock_prices.txt','w') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    for stock, price in today_prices.items():\n",
    "        writer.writerow([stock, price])\n",
    "        \n",
    "# AAPL,90.91\n",
    "# MSFT,41.68\n",
    "# FB,64.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.3\n",
    "\n",
    "#　BeautifulSoup()にHTML文書を渡す。\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "html = requests.get(\"http://www.example.com\").text\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "print(\"==== #元のHTML ====\\n\\n\", html)\n",
    "\n",
    "#　最初の<p>タグを探し出す\n",
    "first_paragraph = soup.find('p')\n",
    "print(\"\\n==== #最初のpタグ内 ====\\n\\n\", first_paragraph)\n",
    "\n",
    "#　textプロパティを通して、タグの文字列部分を取り出す。\n",
    "first_paragraph_text = soup.p.text\n",
    "print(\"\\n==== #最初の一文 ====\\n\\n\", first_paragraph_text)\n",
    "\n",
    "first_paragraph_words = soup.p.text.split()\n",
    "print(\"\\n==== #最初の単語 ====\\n\\n\", first_paragraph_words)\n",
    "\n",
    "#　Tagオブジェクトを辞書として使えば、属性にアクセスできる。\n",
    "#first_paragraph_id = soup.p['id'] #idが無ければKeyError\n",
    "first_paragraph_id2 = soup.p.text.split('id') #idが無ければNoneが返る\n",
    "print(\"\\n==== #属性にアクセス ====\\n\\n\", first_paragraph_id2)\n",
    "\n",
    "#　複数のタグを一度に取り出す。\n",
    "all_paragraphs = soup.find_all('p')\n",
    "paragraphs_with_ids = [p for p in soup('p') if p.get('id')]\n",
    "print(\"\\n==== #複数のタグを一度に取り出す ====\\n\\n\", all_paragraphs)\n",
    "\n",
    "#　注意：複数の<div>の中に同じ<span>があれば、それらをその都度返す\n",
    "#　その様な場合は、さらに工夫が必要\n",
    "spans_inside_divs = [span\n",
    "                    for div in soup('div') #ページ上の<div>要素ごとに繰り返し\n",
    "                    for span in div('span')] #その中の<span>要素で繰り返し\n",
    "\n",
    "print(\"==== #複数の<div>の中の<span>を取り出す ====\", spans_inside_divs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.4.1\n",
    "\n",
    "import json\n",
    "serialized = \"\"\"{ \"title\":\"Data Science Book\", \"author\":\"Joel Grus\", \"publicationYear\":2014, \"topics\":[\"data\",\"science\",\"data science\"]}\"\"\"\n",
    "\n",
    "#　jsonからPython辞書を作る\n",
    "deserialized = json.loads(serialized)\n",
    "if \"data science\" in deserialized[\"topics\"]:\n",
    "    print(deserialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　9.4.2\n",
    "\n",
    "import requests, json\n",
    "endpoint = \"https://api.github.com/users/joelgrus/repos\"\n",
    "repos = json.loads(requests.get(endpoint).text)\n",
    "\n",
    "print(repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from collections import Counter\n",
    "\n",
    "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
    "print(\"==== dates ====\\n\\n\", dates)\n",
    "\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "print(\"\\n==== month ====\\n\\n\", month_counts)\n",
    "\n",
    "weekday_counts = Counter(date.weekday() for date in dates)\n",
    "print(\"\\n==== weekday ====\\n\\n\", weekday_counts)\n",
    "\n",
    "last_5_repositories = sorted(repos,\n",
    "                            key=lambda r: r[\"created_at\"],\n",
    "                            reverse=True)[:5]\n",
    "print(\"\\n==== last_5_repositories ====\\n\\n\", last_5_repositories)\n",
    "\n",
    "last_5_languages = [repo[\"language\"]\n",
    "                   for repo in last_5_repositories]\n",
    "print(\"\\n==== last_5_languages ====\\n\\n\", last_5_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#9.4.3\n",
    "\n",
    "#Rotten Tomatoes\n",
    "#https://www.rottentomatoes.com/\n",
    "\n",
    "#Klout\n",
    "#https://klout.com/home\n",
    "\n",
    "#Yelp\n",
    "#https://www.yelp.com/\n",
    "\n",
    "#IMDb\n",
    "#http://www.imdb.com/\n",
    "\n",
    "#Python API\n",
    "#http://pythonapi.com/\n",
    "\n",
    "#Python for Beginners API list\n",
    "#http://www.pythonforbeginners.com/development/list-of-python-apis/\n",
    "\n",
    "#Programmable Web\n",
    "#https://www.programmableweb.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.5 Twitter API\n",
    "\n",
    "#　事前にtwythonをインストールする\n",
    "#　conda install -c conda-forge twython \n",
    "\n",
    "#　事前にtwitterAPIを取得する。電話番号を登録してあるTwitterアカウントが必要\n",
    "#　https://apps.twitter.com/\n",
    "\n",
    "from twython import Twython\n",
    "\n",
    "#　キー関連は後で削除すること！！！！！！\n",
    "\n",
    "CONSUMER_KEY = '*****'\n",
    "CONSUMER_SECRET = '*****'\n",
    "ACCESS_TOKEN = '*****'\n",
    "ACCESS_TOKEN_SECRET = '*****'\n",
    "\n",
    "twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "# \"data science\"を含むツイートを検索する\n",
    "for status in twitter.search(q='\"data science\"')[\"statuses\"]:\n",
    "    user = status[\"user\"][\"screen_name\"].encode('utf-8')\n",
    "    text = status[\"text\"].encode('utf-8')\n",
    "    print(user,':',text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twython import TwythonStreamer\n",
    "\n",
    "#　データを大域変数に格納するのは稚拙な手法ではあるが、サンプルコードを簡単にできる\n",
    "\n",
    "tweets = []\n",
    "\n",
    "class MyStreamer(TwythonStreamer):\n",
    "    \"\"\"streamとやりとりを行う方法を定義する、TwythonStreamerのサブクラス\"\"\"\n",
    "    \n",
    "    def on_success(self, data):\n",
    "        \"\"\"Twitterがデータを送ってきたらどうするか？\n",
    "        ここでdataは、ツイートを表すPython辞書として渡される\"\"\"\n",
    "        \n",
    "        #　英語のツイートのみを対象とする\n",
    "        if data['lang'] is 'en':\n",
    "            tweets.append(data)\n",
    "            print(\"received tweet #\", len(tweets))\n",
    "            \n",
    "        #　十分なツイートが得られたら終了\n",
    "        if len(tweets) >= 10:\n",
    "            self.disconnect()\n",
    "            \n",
    "    def on_error(self, status_code, data):\n",
    "        print(status_code, data)\n",
    "        self.disconnect()\n",
    "\n",
    "stream = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "#　公開されているツイート（statuses）からキーワード'data'を持つものを収穫する\n",
    "stream.statuses.filter(track='data')\n",
    "\n",
    "#　すべてのツイート（statuses）の中からランダムに収穫を行うには、次のコードを使う\n",
    "#stream.statuses.sample()\n",
    "\n",
    "top_hashtags = Counter(hashtag['text'].lower()\n",
    "                      for tweet in tweets\n",
    "                      for hashtag in tweet[\"entities\"][\"hashtags\"])\n",
    "\n",
    "print(top_hashtags.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
